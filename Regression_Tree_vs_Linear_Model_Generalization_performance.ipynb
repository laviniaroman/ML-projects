{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ames_housing = pd.read_csv(\"../datasets/ames_housing_no_missing.csv\")\n",
    "target_name = \"SalePrice\"\n",
    "data = ames_housing.drop(columns=target_name)\n",
    "target = ames_housing[target_name]\n",
    "numerical_features = [\n",
    "    \"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n",
    "    \"GrLivArea\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\",\n",
    "    \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\",\n",
    "    \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\",\n",
    "]\n",
    "\n",
    "data_numerical = data[numerical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will compare the generalization performance of a decision tree and a linear regression based on two separate predictive models and evaluate them by 10-fold cross-validation.\n",
    "\n",
    "We create the models using sklearn.linear_model.LinearRegression and sklearn.tree.DecisionTreeRegressor with the default parameters for the linear regression and a random_state=0 for the decision.\n",
    "\n",
    "A sklearn.preprocessing.StandardScaler scales the numerical data in the linear regression model.\n",
    "\n",
    "By comparing the cross-validation test scores for both models fold-to-fold, we count the number of times the linear model has a better test score than the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7179515064979931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "linear_model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "cv_results_l = cross_validate(linear_model, data_numerical, target, cv=10, \n",
    "                            return_estimator=True,n_jobs=2)\n",
    "cv_results_l['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6210257285885292"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model = make_pipeline(DecisionTreeRegressor(random_state=0))\n",
    "cv_results_t = cross_validate(tree_model, data_numerical, target, cv=10, \n",
    "                            return_estimator=True,n_jobs=2)\n",
    "cv_results_t['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression is better than decision tree for 9 CV iterations out of 10 folds\n"
     ]
    }
   ],
   "source": [
    "m_l = cv_results_l['test_score']\n",
    "m_t = cv_results_t['test_score']\n",
    "print(\"Linear regression is better than decision tree for \"\n",
    "    f\"{( m_l > m_t).sum()} CV iterations out of 10 folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing the tree model\n",
    "Instead of using the default parameters for the decision tree regressor, we will optimize the max_depth of the tree and vary the max_depth from 1 level up to 15 levels. Fo the evaluation we will use a nested cross-validation with a grid-search (sklearn.model_selection.GridSearchCV), setting cv=10 for both the inner and outer cross-validations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generalization performance of the tree model: 0.6966361061945607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": np.arange(1, 16, 1)}\n",
    "tree_model2 = GridSearchCV(DecisionTreeRegressor(random_state=0), \n",
    "                           param_grid=param_grid, cv=10)\n",
    "cv_results_t2 = cross_validate(tree_model2, data_numerical, target, cv=10, \n",
    "                            return_estimator=True,n_jobs=2)\n",
    "print(\"The generalization performance of the tree model: \" f\"{cv_results_t2['test_score'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5}\n",
      "{'max_depth': 7}\n",
      "{'max_depth': 6}\n",
      "{'max_depth': 6}\n",
      "{'max_depth': 8}\n",
      "{'max_depth': 6}\n",
      "{'max_depth': 7}\n",
      "{'max_depth': 8}\n",
      "{'max_depth': 7}\n",
      "{'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "for search_cv in cv_results_t2[\"estimator\"]:\n",
    "    print(search_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **optimal depth** is ranging from 5 to 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree model including categorical features\n",
    "\n",
    "Instead of using only the numerical features you will now use the entire dataset available in the variable data building a preprocessor. We use an OrdinalEncoder to encode the categorical columns.\n",
    "\n",
    "In addition, we set the max_depth of the decision tree to 7 (fixed, no need to tune it with a grid-search) and valuate this model using cross_validate as in the previous questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\", \n",
    "                                          unknown_value=-1)\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('standard_scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generalization performance of the tree model with numerical and categorical features: 0.7668234519871657\n"
     ]
    }
   ],
   "source": [
    "tree_model3 = make_pipeline(preprocessor, DecisionTreeRegressor(max_depth=7, random_state=0))\n",
    "cv_results_t3 = cross_validate(tree_model3, data, target, cv=10, \n",
    "                            return_estimator=True,n_jobs=2)\n",
    "print(\"The generalization performance of the tree model with numerical and categorical features: \" f\"{cv_results_t3['test_score'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tree model using both numerical and categorical features is better than a tree with optimal depth using only numerical features for 9 CV iterations out of 10 folds.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"A tree model using both numerical and categorical features is better than a \"\n",
    "    \"tree with optimal depth using only numerical features for \"\n",
    "    f\"{sum(cv_results_t3['test_score'] > cv_results_t2['test_score'])} CV \"\n",
    "    \"iterations out of 10 folds.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "A tree model using both numerical and categorical features is better than a tree with optimal depth using only numerical features for 9 CV iterations out of 10 folds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
